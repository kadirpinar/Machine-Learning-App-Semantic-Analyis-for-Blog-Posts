# -*- coding: utf-8 -*-
"""DataMining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NE-d6AfpUozAob8Q1OKHMDDjJcd8ZPMB
"""

#Python'da Zemberek çalıştıramadık. JPype1 yüklenemiyor.

!pip install turkishnlp
!pip install TurkishStemmer

import nltk.data
import numpy as np
import pandas as pd
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from turkishnlp import detector   #Typos
from TurkishStemmer import TurkishStemmer
from collections import Counter        #Check http://www.veridefteri.com/2017/11/20/turkce-metin-islemede-ilk-adimlar/

#NOT: Anlamsız kelimeleri atmak vs.

allBlogs = []    #It will store all blogs content in the the data set
classes = []     #It will store all labels in the data set
allText = ''     #It will store all text content in the data set
folderNames = ['karisik', 'neseli', 'sinirli', 'uzgun']
missingFiles = ['karisik/13.txt', 'karisik/34.txt', 'uzgun/7.txt']

for folderName in folderNames:
  for i in range(1, 41):   
    fileName = folderName + "/" + str(i) + ".txt"
    if fileName in missingFiles:                  #Handling missing files
      continue
    with open(fileName, encoding='ISO 8859-9') as file:
      content = file.read()
      allText += content
      allBlogs.append(content)
      classes.append(folderName)

allBlogs = np.array(allBlogs)  #Creating rank 1 array
df_allBlogs = pd.DataFrame({'Blog': allBlogs, 'Class': classes})  #Creating data structure with labeled axes.(data, rows and columns)
df_allBlogs = df_allBlogs[['Blog', 'Class']]
print(df_allBlogs)

#NOT: soyle söyle gibi ingilzce karakterleri Türkçe karakterlere çevirmek.

nltk.download('punkt')
tokenizer = nltk.data.load('tokenizers/punkt/PY3/turkish.pickle')
nltk.download('stopwords')
turkishStopWords = set(stopwords.words('turkish'))
stemmer = TurkishStemmer()
#Typos
turkishNLPObject = detector.TurkishNLP()
turkishNLPObject.download()
turkishNLPObject.create_word_set()

number=0
numberTokenized=0
numberTypoAndStopWords=0
numberAllAndStemmed=0

def preprocessBlogs(blog):
  #Converting to lowercase characters and removing leading and trailing whitespaces.
  blog = blog.lower()
  blog = blog.strip()
  global number
  global numberTokenized
  global numberTypoAndStopWords
  global numberAllAndStemmed
  number += len(blog.split())
  #Tokenization
  #Tokenized to sentences
  tokenizedToSentences = tokenizer.tokenize(blog)
  #Tokenized to words
  #regex_tokenizer = RegexpTokenizer("[\w']+")   #Words without punctuation
  regex_tokenizer = RegexpTokenizer("[a-zA-ZığüşöçİĞÜŞÖÇ]{2,}")    #Words without punctuation and numbers  #{1,} means + 1 or more repetitions
  tokens = regex_tokenizer.tokenize(''.join(tokenizedToSentences))
  numberTokenized += len(tokens)
  #Correcting typos, yaklaşık 45 bin kelime çok uzun sürmekte
  for i in range(len(tokens)):
    tokens[i] = "".join(turkishNLPObject.auto_correct(turkishNLPObject.list_words(tokens[i])))
  #Removing stop words
  filteredTokens = [token for token in tokens if token not in turkishStopWords]
  numberTypoAndStopWords += len(filteredTokens)
  #Stemming. Türkçe'de birçok kelimeyi yanlış algılamakta. İki adet stemmer kütüphanesi var. İkisi de çok iyi çalışmamakta.
  for i in range(len(filteredTokens)):
    filteredTokens[i] = "".join(stemmer.stem(filteredTokens[i]))
  numberAllAndStemmed += len(filteredTokens)
  #Türkçe için Lemmatizer bulunamamıştır.
  #Reconstructing the document
  blog = ' '.join(filteredTokens)
  return blog

preprocessBlogs = np.vectorize(preprocessBlogs)
preprocessedBlogs = preprocessBlogs(allBlogs)
print(preprocessedBlogs)
print(number, numberTokenized, numberTypoAndStopWords, numberAllAndStemmed)

#Bag of Words

from sklearn.feature_extraction.text import CountVectorizer

#Building vocabulary
BoWVector = CountVectorizer(min_df = 0., max_df = 1.)
BoWMatrix = BoWVector.fit_transform(preprocessedBlogs)

#Fetching all features in BoW_Vector
features = BoWVector.get_feature_names()
#print(features)
# for i in features:
#   print(i)

BoWMatrix = BoWMatrix.toarray()

pd.set_option('display.max_rows', None)
#pd.set_option('display.max_columns', None)
BoW_df = pd.DataFrame(BoWMatrix, columns = features)
#print(BoW_df)

#Dropping features that mentioned less than a specific number from the dataset.
#For better performance (Feature Extraction)
index = 0
for i in BoW_df.sum(axis = 0, skipna = True):
  if i < 5:
    BoW_df.drop(BoW_df.columns[[index]], axis=1, inplace=True)
    index -= 1
  index += 1

print(BoW_df)

#TF, IDF calculations

from sklearn.feature_extraction.text import TfidfVectorizer

Tfidf_Vector = TfidfVectorizer(min_df = 0., max_df = 1., use_idf = True)
Tfidf_Matrix = Tfidf_Vector.fit_transform(preprocessedBlogs)
Tfidf_Matrix = Tfidf_Matrix.toarray()
#print(np.round(Tfidf_Matrix, 3))

#Fetching all features in BoW_Vector
features = Tfidf_Vector.get_feature_names()

Tfidf_df = pd.DataFrame(np.round(Tfidf_Matrix, 3), columns = features)
print(Tfidf_df)

"""# **Decision Tree**"""

#Decision Tree

classes = pd.DataFrame(classes, columns = ['Classes'])

X = BoW_df
y = classes

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) 

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score

print(classification_report(y_test, y_pred))

print('                   Predicted     \nActual')
cmtx = pd.DataFrame(
    confusion_matrix(y_test, y_pred, labels=folderNames), 
    index=folderNames, 
    columns=folderNames
)
print(cmtx)
print('Accuracy', accuracy_score(y_test, y_pred))

"""# **Naive Bayes**"""

#Naive Bayes

classes = pd.DataFrame(classes, columns = ['Classes'])

X = BoW_df
y = classes

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) 

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)

y_pred = gnb.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score

print(classification_report(y_test, y_pred))

print('                   Predicted     \nActual')
cmtx = pd.DataFrame(
    confusion_matrix(y_test, y_pred, labels=folderNames), 
    index=folderNames, 
    columns=folderNames
)
print(cmtx)
print('Accuracy', accuracy_score(y_test, y_pred))

"""# **K-NN Classifier**"""

#K-NN Classifier

classes = pd.DataFrame(classes, columns = ['Classes'])

X = BoW_df
y = classes

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) 

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 3)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score

print(classification_report(y_test, y_pred))

print('                   Predicted     \nActual')
cmtx = pd.DataFrame(
    confusion_matrix(y_test, y_pred, labels=folderNames), 
    index=folderNames, 
    columns=folderNames
)
print(cmtx)
print('Accuracy', accuracy_score(y_test, y_pred))

"""# **SVM**"""

#SVM

classes = pd.DataFrame(classes, columns = ['Classes'])

X = BoW_df
y = classes

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) 

from sklearn.svm import SVC
svclassifier = SVC(kernel = 'linear')
svclassifier.fit(X_train, y_train)

y_pred = svclassifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score

print(classification_report(y_test, y_pred))

print('                   Predicted     \nActual')
cmtx = pd.DataFrame(
    confusion_matrix(y_test, y_pred, labels=folderNames), 
    index=folderNames, 
    columns=folderNames
)
print(cmtx)
print('Accuracy', accuracy_score(y_test, y_pred))

"""# **Random Forest**"""

#Random Forest
#NOT: Parametreleri kontrol et

classes = pd.DataFrame(classes, columns = ['Classes'])

X = BoW_df
y = classes

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) 

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(max_depth = 3, n_estimators=10)
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score

print(classification_report(y_test, y_pred))

print('                   Predicted     \nActual')
cmtx = pd.DataFrame(
    confusion_matrix(y_test, y_pred, labels=folderNames), 
    index=folderNames, 
    columns=folderNames
)
print(cmtx)
print('Accuracy', accuracy_score(y_test, y_pred))

"""# **K-Means Clustering**"""

#K-Means Clustering

classes = pd.DataFrame(classes, columns = ['Classes'])

X = BoW_df
y = classes

from sklearn.cluster import KMeans
    
# Specify the number of clusters (4) and fit the data X
kmeans = KMeans(n_clusters=4, random_state=0).fit(X)

# Get the cluster labels
print(kmeans.labels_)

results = kmeans.labels_.tolist()

for i in range(len(results)):
  if results[i] == 0:
    results[i] = 'karisik'
  elif results[i] == 1:
    results[i] = 'neseli'
  elif results[i] == 2:
    results[i] = 'sinirli'
  elif results[i] == 3:
    results[i] = 'uzgun'

print(results)

score = 0
for i in range(len(X)):
  if results[i] == str(y.iloc[:,-1][i]):
    score += 1

print("Accuracy: ", round(score / len(X), 2))

"""# **Hieararchial Clustering**"""

#Hieararchial Clustering

classes = pd.DataFrame(classes, columns = ['Classes'])

X = BoW_df
y = classes

import matplotlib.pyplot as plt


import scipy.cluster.hierarchy as shc

plt.figure(figsize=(12, 9))
plt.title("Ruh Halleri")
dend = shc.dendrogram(shc.linkage(X, method='ward'))